{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>2 important things in data augmentation -\n",
    "\n",
    "*a. What kind of augmentation is applied ? e.g. horizontal flip etc*\n",
    "\n",
    "*b. How many augmentations are applied, usually 1 augment for each image so to avoid duplication.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*\n",
    "\n",
    "Data Augmentation is only applied during training. It has no role during testing. \n",
    "\n",
    "As seen below augmentation is only created for training dataset & not test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen=image.ImageDataGenerator(vertical_flip=True, \n",
    "                                   horizontal_flip=True,\n",
    "                                   rotation_range=30,\n",
    "                                   rescale=1/255) \n",
    "\n",
    "test_gen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_batch_size = 35   # batch size 35 as fully divisible for 735 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No of batches = 735/35 \n",
    "#              = 21 batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<font color=blue>Note*\n",
    "    \n",
    "In case u had validation split as well as shown below in commented code, batch_size could be different for train & validation data. v_batch_size could be different like v_train_size and v_val_size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_set,epochs=5,\n",
    "#           steps_per_epoch = test_set.samples/v_batch_size,\n",
    "#           validation_data =val_set,\n",
    "#           validation_steps=val_set.samples/v_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually train data is shuffled while test data is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 735 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data=train_gen.flow_from_directory('data/waffle_pancakes_ds/train',\n",
    "                                          shuffle=True,\n",
    "                                          seed=0,\n",
    "                                          batch_size=v_batch_size,\n",
    "                                          target_size=(224,224),\n",
    "                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tB-6e3_-A3h1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 389 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data=test_gen.flow_from_directory('data/waffle_pancakes_ds/test',\n",
    "                                        shuffle=False,\n",
    "                                        batch_size = v_batch_size,\n",
    "                                        target_size=(224,224),\n",
    "                                        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 01:11:28.594251: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64,(3,3),input_shape = (224,224,3), activation = 'relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation = 'relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())  \n",
    " \n",
    "model.add(Dense( activation = 'relu', units=64))\n",
    "model.add(Dense( activation = 'sigmoid', units=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 43264)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2768960   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,844,673\n",
      "Trainable params: 2,844,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - train_data is an iterator to the training dataset\n",
    "    - In every batch the training data flows from the directory specified and is augmented.\n",
    "    - Here every batch comprises 35(batch_size) augmented images of the training data\n",
    "    - set the steps_per_epoch argument of fit method to n_train_samples / batch_size, where n_train_samples is the total number of training data you have.\n",
    "    - This will ensure that in each epoch, each training sample is augmented only once and therefore n_train_samples transformed images will be generated in each epoch.\n",
    "    - In the absence of this argument, different augmented versions of the same image might be passed to training which results in training with duplicate copies of the same image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please Note :\n",
    "\n",
    "    - After augmentation the number of training images does not increase per epoch\n",
    "    - Different transformation is applied to each image in every epoch\n",
    "    - Hence if we train our model for, 5 epochs, we have used 5 different augmented versions of each original image in training (or 100 * 5 = 500 different images in the whole training, instead of using just the 100 original images in the whole training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<font color=blue>steps_per_epoch is the number of batches*\n",
    "    \n",
    "This argument will ensure all 735 images r augmented once & only once in an epoch if samples/batch_size is perfectly divisible. \n",
    "    \n",
    "e.g. if we take batch size as 35, so 735/35 = 21 batches, all training data used and augmented once.\n",
    "    \n",
    "e.g. if we take batch size as 30, so 735/30 = 24.5 batches. After floor division 24. So 24*30 = 720 images will be fully used while remaining 15 images will not be used so ur whole training dataset is not augmented.\n",
    "    \n",
    "In case not fully divisible, try to take closest possible batch size which covers most of the samples.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "21/21 [==============================] - 31s 1s/step - loss: 0.5152 - accuracy: 0.7673\n",
      "Epoch 2/2\n",
      "21/21 [==============================] - 32s 1s/step - loss: 0.5058 - accuracy: 0.7673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93dc3ffe50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,steps_per_epoch= train_data.samples//v_batch_size, epochs=2)\n",
    "\n",
    "# floor division // it will take lower bound. For scenarios when batch size not a\n",
    "# factor of training samples better to do floor division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy/performance on unseen data is expected to be more when model trained with augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 5s 394ms/step - loss: 0.8321 - accuracy: 0.6041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8320676684379578, 0.6041131019592285]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 5s 441ms/step\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[30].argmax() == test_data.classes[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[150].argmax() == test_data.classes[150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note on model.fit*\n",
    "\n",
    "In case we would have taken number of batches as 35 & batch size 35 then r samples would exceed 735 & we would get a warning 'Your input ran out of data' while fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "21/35 [=================>............] - ETA: 21s - loss: 0.5262 - accuracy: 0.7673WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 70 batches). You may need to use the repeat() function when building your dataset.\n",
      "35/35 [==============================] - 33s 908ms/step - loss: 0.5262 - accuracy: 0.7673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93dc3f84c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,steps_per_epoch= 35, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DataGeneration_Augmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
