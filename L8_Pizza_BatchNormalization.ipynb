{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network (GOOD PIZZA  vs BURNT PIZZA)\n",
    "\n",
    "# Building the CNN\n",
    "\n",
    "#Importing Libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "\n",
    "image_width=150\n",
    "image_height=150\n",
    "\n",
    "# train_data_dir=r'PIZZA/Train'\n",
    "# test_data_dir=r'PIZZA/Test'\n",
    "# validation_data_dir=r'PIZZA/Val'\n",
    "\n",
    "#train_sample=30\n",
    "#validation_sample=25\n",
    "test_sample=50\n",
    "\n",
    "#epochs=30\n",
    "batch_size=12\n",
    "test_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape= (3,image_width,image_height)\n",
    "else :\n",
    "    input_shape= (image_width,image_height,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "train_datagen= ImageDataGenerator(rotation_range=40,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  rescale=1./255,\n",
    "                                  horizontal_flip=True,\n",
    "                                 fill_mode='nearest');\n",
    "\n",
    "validation_datagen= ImageDataGenerator(rescale=1./255)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 566 images belonging to 2 classes.\n",
      "Found 221 images belonging to 2 classes.\n",
      "Found 199 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_genarator= train_datagen.flow_from_directory('data/pizza_ds/Train',\n",
    "                                                   target_size=(image_width,image_height),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode='binary')\n",
    "\n",
    "\n",
    "validation_genarator= validation_datagen.flow_from_directory('data/pizza_ds/Val',\n",
    "                                                        target_size=(image_width,image_height),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='binary')\n",
    "\n",
    "\n",
    "test_genarator= test_datagen.flow_from_directory('data/pizza_ds/Test',\n",
    "                                                 target_size=(image_width,image_height),\n",
    "                                                 batch_size=test_size,\n",
    "                                                 class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 10:23:56.812950: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#build model\n",
    "\n",
    "# Initialising the CNN\n",
    "\n",
    "model = Sequential()\n",
    "# Convolution\n",
    "model.add(Conv2D(32,(3,3),input_shape=input_shape, activation='relu'))\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())  # these batch normalized o/p's tend to converge faster\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "model.add(Conv2D(32,(3,3),input_shape=input_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding a third convolutional layer\n",
    "model.add(Conv2D(64,(3,3),input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# Full connection\n",
    "model.add(Dense(64, activation='relu',))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training....\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 6s 111ms/step - loss: 0.8736 - accuracy: 0.6444 - val_loss: 0.6215 - val_accuracy: 0.7778\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 5s 105ms/step - loss: 0.5620 - accuracy: 0.7166 - val_loss: 0.8208 - val_accuracy: 0.5602\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 5s 106ms/step - loss: 0.5099 - accuracy: 0.7437 - val_loss: 1.2408 - val_accuracy: 0.5648\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 5s 105ms/step - loss: 0.4979 - accuracy: 0.7563 - val_loss: 1.6060 - val_accuracy: 0.5741\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 5s 106ms/step - loss: 0.4842 - accuracy: 0.7455 - val_loss: 1.6554 - val_accuracy: 0.5694\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 5s 106ms/step - loss: 0.4815 - accuracy: 0.7617 - val_loss: 1.1282 - val_accuracy: 0.5602\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 5s 105ms/step - loss: 0.4754 - accuracy: 0.7617 - val_loss: 2.5213 - val_accuracy: 0.5648\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 5s 106ms/step - loss: 0.4512 - accuracy: 0.7762 - val_loss: 1.5118 - val_accuracy: 0.5648\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 5s 106ms/step - loss: 0.4486 - accuracy: 0.7870 - val_loss: 1.0175 - val_accuracy: 0.6481\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 5s 106ms/step - loss: 0.4522 - accuracy: 0.7690 - val_loss: 0.3906 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5eef7dee0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fitting the CNN to the images\n",
    "\n",
    "print('starting training....')\n",
    "\n",
    "model.fit(train_genarator,steps_per_epoch=train_genarator.samples//batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=validation_genarator,\n",
    "          validation_steps=validation_genarator.samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 119ms/step - loss: 0.4977 - accuracy: 0.8141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4977009892463684, 0.8140703439712524]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_genarator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished!!\n"
     ]
    }
   ],
   "source": [
    "print('training finished!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_inference(img):\n",
    "    im = image.img_to_array(img)\n",
    "    img1 = im/255\n",
    "    img1 = img1.reshape((1,150,150,3))\n",
    "    prediction = model.predict(img1)\n",
    "    print(prediction)\n",
    "    if prediction > 0.5:\n",
    "        print('Good Pizza')\n",
    "    else:\n",
    "        print('Burnt Pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n",
      "[[0.82201415]]\n",
      "Good Pizza\n"
     ]
    }
   ],
   "source": [
    "img1 = image.load_img(r'data/pizza_ds/Test/Good/good (35).jpg',target_size=(150,150))\n",
    "obtain_inference(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[0.5833607]]\n",
      "Good Pizza\n"
     ]
    }
   ],
   "source": [
    "img2 = image.load_img(r'data/pizza_ds/Test/Burnt/12.jpg',target_size=(150,150))\n",
    "obtain_inference(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Batch Normalization\n",
    "\n",
    "    1.Every Batch of data from a layer is normalized with mean=0 and std=1 before its fed into subsequent layers. Every neuron has an adder & activation, so o/p of neurons u can standardize that with mean=0 & std=1 across a batch in a particular layer.\n",
    "    2. The mean and standard deviation (or variance) are calculated for the 32 samples for each neuron individually, not for all neurons together. So if you have N neurons, you'll end up with N different means and N different variances (one for each neuron) based on the outputs of the 32 samples in the mini-batch.\n",
    "    3.This ensures faster convergence and speeds up the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
